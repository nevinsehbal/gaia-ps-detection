{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of the Dataloader for the Seismic Dataset ##\n",
    "\n",
    "This notebook shows how to convert hdf5 file to our custom seismic dataset, ready to be used in our model training, validation and testing.\n",
    "\n",
    "1. First import the customDataset class, which includes utility functions to load the dataset from the hdf5 file, windowing, and labelling the data.\n",
    "2. Next, import te printCustom function to print in different modes (info, success, error)\n",
    "3. Next, import Datloader and random_split from torch utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customDataset\n",
    "from logUtils import printCustom\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset from the hdf5 file\n",
    "- Parameters:\n",
    "   - file_path (str): path to the hdf5 file\n",
    "   - seconds (int): duration of the window in samples (options: 1500 (15 sec), 3000 (30 sec), 6000 (60 sec), 10000 (100 sec))\n",
    "   - window_size (int): size of the window in samples\n",
    "   - hopping_size (int): size of the hop in samples\n",
    "   - verbose (bool): print information about the dataset\n",
    "- Returns:\n",
    "    - dataset (SeismicDataset): seismic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m---------------- Dataset information: ----------------\u001b[0m\n",
      "\u001b[96m[I] Sample rate: 100 Hz\u001b[0m\n",
      "\u001b[96m[I] Chosen window size:900\u001b[0m\n",
      "\u001b[96m[I] Hopping size:300\u001b[0m\n",
      "\u001b[96m[I] Each sample is window_size/sample rate seconds long. Each sample has 3 channels.\u001b[0m\n",
      "\u001b[96m[I] Dataset values shape is: [num_samples, num_window, num_channel, (height) 1, (width) num_sample_points]\u001b[0m\n",
      "\u001b[96m[I] Dataset value shape: torch.Size([1575, 3, 3, 1, 900])\u001b[0m\n",
      "\u001b[96m[I] Dataset labels shape is: [num_samples, 4], where the 4 values are [p_idx, s_idx, p_confidence, s_confidence]\u001b[0m\n",
      "\u001b[96m[I] Dataset label shape:torch.Size([4725, 4])\u001b[0m\n",
      "\u001b[96m[I] One sample shape: torch.Size([3, 3, 1, 900])\u001b[0m\n",
      "\u001b[96m[I] One label shape: torch.Size([4])\u001b[0m\n",
      "\u001b[96m[I] An example sample's label: tensor([4.7900e+02, 1.0180e+03, 1.0000e+00, 0.0000e+00])\u001b[0m\n",
      "\u001b[96m[I] ----------------------------------------------------\u001b[0m\n",
      "\u001b[96m[I] First sample in the dataset:\u001b[0m\n",
      "\u001b[96m[I] Data shape: torch.Size([3, 3, 1, 900])\u001b[0m\n",
      "\u001b[96m[I] Label shape: torch.Size([4])\u001b[0m\n",
      "\u001b[96m[I] Label: tensor([-1., -1.,  0.,  0.])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = customDataset.get_dataset(file_path=\"/Users/nevinsehbal/Documents/workspace/ps-paper-convlstm/gaia-ps-detection/afad_to_hdf5/afad_to_hdf5.hdf5\", \n",
    "                                    seconds = 1500 , window_size=900, hopping_size=300, verbose=True)\n",
    "\n",
    "printCustom(\"info\",\"First sample in the dataset:\")\n",
    "for i, (data, label) in enumerate(dataset):\n",
    "    printCustom(\"info\",\"Data shape: \" + str(data.shape))\n",
    "    printCustom(\"info\",\"Label shape: \"+ str(label.shape))\n",
    "    printCustom(\"info\",\"Label: \"+str(label))\n",
    "    if i == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly split the dataset into train, validation and test sets \n",
    "For this:\n",
    "1. First, define the ratio of the TRAIN_RATIO and VALIDATION_RATIO. TEST_RATIO will automatically be 1-(train+val) \n",
    "2. Then set the seed manually for reproducibility.\n",
    "3. Next, define TRAIN_BATCH_SIZE and TEST_BATCH_SIZE. Validation batch size will be same as train batch size.\n",
    "4. Finally, random split the dataset into train, validation, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO, VALIDATION_RATIO = 0.8, 0.1\n",
    "# check if the sum of the ratios is not more than 1, if it is, raise an exception\n",
    "assert TRAIN_RATIO + VALIDATION_RATIO <= 1, \"The sum of the ratios should be less than or equal to 1\"\n",
    "train_size = int(TRAIN_RATIO * len(dataset))\n",
    "val_size = int(VALIDATION_RATIO * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "# Then, create train, validation and test dataloaders\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# random_split function will split the dataset randomly\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using torch Dataloader class, create train, validation, test dataloaders with SeismicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[S] Dataloaders are created.\u001b[0m\n",
      "\u001b[92m[S] Train dataloader size:1264\u001b[0m\n",
      "\u001b[92m[S] Validation dataloader size:160\u001b[0m\n",
      "\u001b[92m[S] Test dataloader size:158\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "printCustom(\"success\",\"Dataloaders are created.\")\n",
    "printCustom(\"success\",\"Train dataloader size:\"+str(len(train_dataloader)*TRAIN_BATCH_SIZE))\n",
    "printCustom(\"success\",\"Validation dataloader size:\"+str(len(val_dataloader)*TRAIN_BATCH_SIZE))\n",
    "printCustom(\"success\",\"Test dataloader size:\"+str(len(test_dataloader)*TEST_BATCH_SIZE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
